{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "479bcedb-b14c-41f7-baca-bc5e77d9d2a9",
   "metadata": {},
   "source": [
    "# CNN + KAN + MNIST + NTK + Few Shot\n",
    "Purpose: Fit a CNN + KAN, but with Few Shot to the CIFAR10 dataset, for benchmarking the KAN performance.\n",
    "\n",
    "Furthermore, the PyTorch Lightning library is used for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5906942-d959-4742-a7f5-3f0d9d1eb9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from random import shuffle\n",
    "import gc\n",
    "\n",
    "import sys\n",
    "sys.path.append('../convkans/kan_convolutional')\n",
    "from KANLinear import *\n",
    "\n",
    "# Setup Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Setup Randomness -- https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "L.seed_everything(42, workers=True)\n",
    "\n",
    "# CUDA Efficiency\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298f7983-dee7-49b7-a4b8-87218d313bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "One List Size: 62487500, Zero List Size: 62487500\n"
     ]
    }
   ],
   "source": [
    "# Dataset Setup -- Inspired by Hugo's Dataset Reformatting\n",
    "batch_size = 64\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = CIFAR10(\"./temp/\", train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(\"./temp/\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Step 1 -- Select which 5 classes to train on at random, and which 5 to test on.\n",
    "class_train = np.random.choice(np.arange(0, 10), 5, replace=False)\n",
    "class_test = np.setdiff1d(np.arange(0, 10), class_train)\n",
    "\n",
    "# Step 2 -- Convert the train and test sets accordingly (individual lists for each class).\n",
    "temp_train = []\n",
    "for z in range(10):\n",
    "    temp_train.append([])\n",
    "    for x, y in train_dataset:\n",
    "        if y == z:\n",
    "            temp_train[z].append((x, y))\n",
    "    shuffle(temp_train[z])\n",
    "\n",
    "temp_test = []\n",
    "for z in range(10):\n",
    "    temp_test.append([])\n",
    "    for x, y in test_dataset:\n",
    "        if y == z:\n",
    "            temp_test[z].append((x, y))\n",
    "    shuffle(temp_test[z])\n",
    "\n",
    "# Step 3 -- Convert them to similarity comparison format.\n",
    "\n",
    "## Training + Validation Setup\n",
    "\n",
    "train_limit = 500000 # Limit in total\n",
    "validation_limit = 20000 # Limit in total\n",
    "\n",
    "def create_training_and_validation(): # Created function to force garbage collection\n",
    "    total_dataset2_one = []\n",
    "    for z in class_train:\n",
    "        for i in range(len(temp_train[z])):\n",
    "            for j in range(i + 1, len(temp_train[z])):\n",
    "                x1, y1 = temp_train[z][i]\n",
    "                x2, y2 = temp_train[z][j]\n",
    "                total_dataset2_one.append((x1, x2, np.float32(1)))\n",
    "    \n",
    "    max_val = ((len(temp_train[0])**2 - len(temp_train[0]))//2)*(len(class_train)**2 - len(class_train))\n",
    "    mod_val = max_val // len(total_dataset2_one)\n",
    "    total_dataset2_zero = []\n",
    "    count = 0\n",
    "    for z1 in class_train:\n",
    "        for z2 in class_train:\n",
    "            if z1 != z2:\n",
    "                for i in range(len(temp_train[z1])):\n",
    "                    for j in range(i + 1, len(temp_train[z2])):\n",
    "                        x1, y1 = temp_train[z1][i]\n",
    "                        x2, y2 = temp_train[z2][j]\n",
    "                        if len(total_dataset2_zero) < len(total_dataset2_one) and count % mod_val == 0:\n",
    "                            total_dataset2_zero.append((x1, x2, np.float32(0)))\n",
    "                        count += 1\n",
    "    \n",
    "    shuffle(total_dataset2_one)\n",
    "    shuffle(total_dataset2_zero)\n",
    "    \n",
    "    print(f'One List Size: {len(total_dataset2_one)}, Zero List Size: {len(total_dataset2_zero)}')\n",
    "    \n",
    "    train_dataset2 = []\n",
    "    validation_dataset2 = []\n",
    "    for i in range(train_limit//2):\n",
    "        train_dataset2.append(total_dataset2_one[i])\n",
    "        train_dataset2.append(total_dataset2_zero[i])\n",
    "    \n",
    "    for i in range(train_limit//2, train_limit//2 + validation_limit//2):\n",
    "        validation_dataset2.append(total_dataset2_one[i])\n",
    "        validation_dataset2.append(total_dataset2_zero[i])\n",
    "\n",
    "    while len(total_dataset2_one) != 0:\n",
    "        total_dataset2_one.pop()\n",
    "\n",
    "    while len(total_dataset2_zero) != 0:\n",
    "        total_dataset2_zero.pop()\n",
    "\n",
    "    del total_dataset2_one\n",
    "    del total_dataset2_zero\n",
    "\n",
    "    return train_dataset2, validation_dataset2\n",
    "\n",
    "train_dataset2, validation_dataset2 = create_training_and_validation()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "## Testing Setup\n",
    "one_shot_test = []\n",
    "\n",
    "for z in class_test:\n",
    "    one_shot_test.append(temp_test[z][len(temp_test[z]) - 1])\n",
    "\n",
    "one_shot_query = []\n",
    "for z in class_test:\n",
    "    for x, y in temp_test[z]:\n",
    "        one_shot_query.append((x, y))\n",
    "    one_shot_query.pop() # Exclude the last value, since it is used in the one_shot_test set.\n",
    "\n",
    "# Step 4: Convert to Lightning Compatible Datasets\n",
    "class LCTrainDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1, x2, y = self.dataset[idx]\n",
    "        return x1, x2, y\n",
    "\n",
    "class LCTestDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        return x, y\n",
    "\n",
    "train_loader = DataLoader(LCTrainDataset(train_dataset2), batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "validation_loader = DataLoader(LCTrainDataset(validation_dataset2), batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "test_loader = DataLoader(LCTestDataset(one_shot_query), batch_size=batch_size, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcf44a29-2a95-4e65-9372-1550ca2c8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Declaration -- Siamese Network, implementation inspired by the video by Shusen Wang: https://www.youtube.com/watch?v=4S-XDefSjTM\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=2, padding=0, bias=False),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=0, bias=False),\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "\n",
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.kan = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            KAN(layers_hidden=[3136, 128], grid_size=2, spline_order=2),\n",
    "            nn.Dropout(p=0.5),\n",
    "            KAN(layers_hidden=[128, 1], grid_size=2, spline_order=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.kan(x)\n",
    "\n",
    "class SiameseNet(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f = Embedding()\n",
    "        self.dl = DenseLayer()\n",
    "        self.out = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1e = self.f(x1)\n",
    "        x2e = self.f(x2)\n",
    "        z = torch.abs(x1e - x2e)\n",
    "        return torch.squeeze(self.out(self.dl(z)))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1, x2, y = batch\n",
    "        y_pred = self(x1, x2)\n",
    "        loss = F.mse_loss(y_pred, y) # MSE Loss works better for NTK\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x1, x2, y = batch\n",
    "        y_pred = self(x1, x2)\n",
    "        loss = F.mse_loss(y_pred, y) # MSE Loss works better for NTK\n",
    "        self.log(\"validation_loss\", loss, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x1, y = batch\n",
    "        list_y_pred = np.array([self(x1, torch.unsqueeze(x2, 0).expand(len(x1), -1, -1, -1).to(device)).cpu() for x2, y2 in one_shot_test])\n",
    "        list_y_pred = torch.Tensor(list_y_pred)\n",
    "        list_y_pred = torch.argmax(list_y_pred, dim=0)\n",
    "        list_y_pred = torch.squeeze(list_y_pred)\n",
    "        list_y_predicted = torch.Tensor(np.array([one_shot_test[x][1] for x in list_y_pred])).to(device)\n",
    "        accuracy = torch.sum(torch.eq(list_y_predicted, y)) / len(y)\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self(batch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, weight_decay=5e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89b2034b-39fd-4e6a-8a75-7c611e697cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type       | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | f    | Embedding  | 18.9 K | train\n",
      "1 | dl   | DenseLayer | 2.4 M  | train\n",
      "2 | out  | Sigmoid    | 0      | train\n",
      "--------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.713     Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                  | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4201181c194c3ba1e0f52494ea695d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                         | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                       | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                       | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                       | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                       | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                       | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                       | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                       | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                       | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                       | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                       | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1613cc5f1e0475590b937f205c63ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.20380380749702454    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.20380380749702454   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.20380380749702454}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train + Test + Results\n",
    "model = SiameseNet()\n",
    "trained_model = L.Trainer(max_epochs=10, deterministic=True, logger=CSVLogger(\"logs\", name=\"CIFAR10FewCNNKANNTK\"))\n",
    "trained_model.fit(model, train_loader, validation_loader)\n",
    "trained_model.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46fc063-69f9-4b6c-b828-0532713513f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply NTK\n",
    "%run -i './introduction_code_modified.py'\n",
    "\n",
    "def cross_entropy_loss_batch(y_hat, y):\n",
    "    return F.cross_entropy(y_hat, y, reduction='none')\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "ntk_model = GaussianFit(model=model, device=device, noise_var=0.0)\n",
    "ntk_model.fit(ntk_loader, optimizer, MSELoss_batch) # MSELoss seems to work better for NTK Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d581c53c-624a-4cce-ac29-9802c4b720b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ntk_acc(model, dataloader):\n",
    "    res = 0.0\n",
    "    sumlength = 0\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    for it in iter(dataloader):\n",
    "        x, y = it\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        sumlength += len(x)\n",
    "        res += (torch.argmax(model.forward(x), dim=1) == torch.argmax(y, dim=1)).sum()\n",
    "    model.train()\n",
    "    return res / sumlength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48403686-91b7-4404-9e9d-ee7befcc3d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'NTK Accuracy: {check_ntk_acc(ntk_model, test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa1a89-35c5-460f-b5a1-d94202ea2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: True Value, C: Predicted Value\n",
    "def make_predict_matrix(model, dataloader):\n",
    "    res = np.zeros(shape=(10, 10), dtype=int)\n",
    "    for it in iter(dataloader):\n",
    "        x, y = it\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        x_arg = torch.argmax(model.forward(x), dim=1)\n",
    "        y_arg = torch.argmax(y, dim=1)\n",
    "        for i in range(len(x_arg)):\n",
    "            res[y_arg[i], x_arg[i]] += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed94fc-995a-4354-8f6b-258a46283921",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_predict_matrix(ntk_model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580942b-37cd-445c-b276-3a7c856b8633",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_predict_matrix(model, test_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
