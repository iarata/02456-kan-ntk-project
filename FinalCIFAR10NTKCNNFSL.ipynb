{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "479bcedb-b14c-41f7-baca-bc5e77d9d2a9",
   "metadata": {},
   "source": [
    "# CNN + NTK + CIFAR10 + FSL\n",
    "Purpose: Fit a NTK to a CNN intended for the CIFAR10 dataset. The purpose is to establish a baseline for the CIFAR10 experiments.\n",
    "\n",
    "This is the FSL variant, which means that the model from the accuracy variant is now trained using only very limited data.\n",
    "\n",
    "Furthermore, the PyTorch Lightning library is used for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5906942-d959-4742-a7f5-3f0d9d1eb9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 68812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured CUDA Precision\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from torchvision import transforms\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('convkans/kan_convolutional')\n",
    "from KANLinear import *\n",
    "\n",
    "# Setup Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Setup Randomness -- https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "L.seed_everything(68812, workers=True) # <--- Adjust the seed here for each trial run for the 5 seed values [42, 998244353, 10122024, 1000000007, 68812].\n",
    "\n",
    "# CUDA Efficiency\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    print(\"Configured CUDA Precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298f7983-dee7-49b7-a4b8-87218d313bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Dataset Setup -- Inspired by Hugo's Dataset Reformatting\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = CIFAR10(\"./Experiments/temp/\", train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(\"./Experiments/temp/\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_size = int(len(train_dataset) * 0.90)\n",
    "val_size = int(len(train_dataset)) - train_size # Quite low validation size, but this is done to allow for greater model accuracy.\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Reformatted, due to odd issues when using NTK on it\n",
    "class LCDataset(Dataset): # Lightning Compatible Dataset\n",
    "    def __init__(self, dataset, num_classes, limit_per_class=-1, randomise=False):\n",
    "        self.limit_per_class = limit_per_class\n",
    "        self.num_classes = num_classes\n",
    "        if self.limit_per_class != -1:\n",
    "            sub = list(np.random.permutation(np.arange(len(dataset)))) # Generate a random permutation of the dataset.\n",
    "            # Select the first x of each class\n",
    "            sub2 = []\n",
    "            sub_count = [0 for x in range(self.num_classes)]\n",
    "            for x in sub:\n",
    "                temp_x, temp_y = dataset[x]\n",
    "                if sub_count[temp_y] != self.limit_per_class:\n",
    "                    sub_count[temp_y] += 1\n",
    "                    sub2.append(x)\n",
    "            for x in sub_count:\n",
    "                assert x == self.limit_per_class\n",
    "            assert len(sub2) == self.num_classes * self.limit_per_class\n",
    "            self.dataset = Subset(dataset, sub2)\n",
    "        elif randomise == True: # Idea: Take 2 values each to form a pair to check against (FSL case)\n",
    "            sub = list(np.random.permutation(np.arange(len(dataset)))) # Generate a random permutation of the dataset.\n",
    "            self.dataset = Subset(dataset, sub)\n",
    "        else:\n",
    "            self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        y_one_hot = torch.zeros(self.num_classes)\n",
    "        y_one_hot[y] = 1\n",
    "        return x, y_one_hot\n",
    "\n",
    "batch_size = 64\n",
    "ntk_loader = DataLoader(LCDataset(train_dataset, num_classes=10, limit_per_class=15), batch_size=batch_size, num_workers=10)\n",
    "train_loader = DataLoader(LCDataset(train_set, num_classes=10), batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "val_loader = DataLoader(LCDataset(val_set, num_classes=10), batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "test_loader = DataLoader(LCDataset(test_dataset, num_classes=10), batch_size=batch_size, num_workers=10)\n",
    "similarity_loader = DataLoader(LCDataset(test_dataset, num_classes=10, randomise=True), batch_size=2, num_workers=10) # Each batch constitutes a pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcf44a29-2a95-4e65-9372-1550ca2c8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Declaration\n",
    "class ClassicCNN(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Conv2d(3, 16, kernel_size=2, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(16, 64, kernel_size=3, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(64, 96, kernel_size=3, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.GELU(),\n",
    "            nn.Flatten(1),\n",
    "            nn.Dropout(p=0.50),\n",
    "            nn.Linear(3456, 64, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.50),\n",
    "            nn.Linear(64, 10, bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        v1 = torch.argmax(y_pred, dim=1)\n",
    "        v2 = torch.argmax(y, dim=1)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        accuracy = torch.sum(torch.eq(v1, v2)) / len(y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", accuracy, prog_bar=True) # Observe model accuracy with time\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        v1 = torch.argmax(y_pred, dim=1)\n",
    "        v2 = torch.argmax(y, dim=1)\n",
    "        accuracy = torch.sum(torch.eq(v1, v2)) / len(y)\n",
    "        self.log(\"accuracy\", accuracy) # Accuracy for Accuracy vs. Parameter Experiment\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self(batch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89b2034b-39fd-4e6a-8a75-7c611e697cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type       | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | net  | Sequential | 286 K  | train\n",
      "--------------------------------------------\n",
      "286 K     Trainable params\n",
      "0         Non-trainable params\n",
      "286 K     Total params\n",
      "1.148     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=0` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861e5aa2cfb5462d8bc13f3e2335ae37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.11289999634027481    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.11289999634027481   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.11289999634027481}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train + Test + Results\n",
    "model = ClassicCNN()\n",
    "trained_model = L.Trainer(max_epochs=0, deterministic=True, logger=CSVLogger(save_dir=\"Experiments/logs\", name=\"CIFAR10NTKCNNFSL\", version=\"fin\"), callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)], log_every_n_steps=1)\n",
    "trained_model.fit(model, train_loader, val_loader)\n",
    "trained_model.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f298b9c0-33f0-4ad9-963d-2849317d3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply NTK\n",
    "%run -i './Experiments/introduction_code_modified.py'\n",
    "\n",
    "def cross_entropy_loss_batch(y_hat, y):\n",
    "    return F.cross_entropy(y_hat, y, reduction='none')\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "ntk_model = GaussianFit(model=model, device=device, noise_var=0.0)\n",
    "ntk_model.fit(ntk_loader, optimizer, MSELoss_batch) # MSELoss seems to work better for NTK Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1809d35b-62bf-4cbb-b619-60e184a5bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ntk_acc(model, dataloader):\n",
    "    res = 0.0\n",
    "    sumlength = 0\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    for it in iter(dataloader):\n",
    "        x, y = it\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        sumlength += len(x)\n",
    "        res += (torch.argmax(model.forward(x), dim=1) == torch.argmax(y, dim=1)).sum()\n",
    "    model.train()\n",
    "    return res / sumlength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6c70d1-5fd0-406b-82bb-7fa1bc01abd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTK Accuracy: 0.10520000010728836\n"
     ]
    }
   ],
   "source": [
    "print(f'NTK Accuracy: {check_ntk_acc(ntk_model, test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21dc34dd-ea5a-4c49-ae2c-aa4e09e3746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fsl_similarity(model, dataloader): # Must use a batch size of 2 for this to work\n",
    "    res = 0.0\n",
    "    sumlength = 0\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    for it in iter(dataloader):\n",
    "        x, y = it\n",
    "        if len(x) != 2:\n",
    "            print(f'Broke loop at: {sumlength} runs.')\n",
    "            break\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        fx = model.forward(x)\n",
    "        if (torch.argmax(fx[0]) != torch.argmax(fx[1]) and torch.argmax(y[0]) != torch.argmax(y[1])) or (torch.argmax(fx[0]) == torch.argmax(fx[1]) and torch.argmax(y[0]) == torch.argmax(y[1])):\n",
    "            res += 1\n",
    "        sumlength += 1\n",
    "    return res, sumlength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c8ff85-fe0f-4fad-a8f1-de1a9bb65e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTK FSL Score: 0.396 on 5000 random samples\n"
     ]
    }
   ],
   "source": [
    "resntk, sumlengthntk = check_fsl_similarity(ntk_model, similarity_loader)\n",
    "print(f'NTK FSL Score: {resntk / sumlengthntk} on {sumlengthntk} random samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cfa1a89-35c5-460f-b5a1-d94202ea2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: True Value, C: Predicted Value\n",
    "def make_predict_matrix(model, dataloader):\n",
    "    res = np.zeros(shape=(10, 10), dtype=int)\n",
    "    model = model.to(device)\n",
    "    for it in iter(dataloader):\n",
    "        x, y = it\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        x_arg = torch.argmax(model.forward(x), dim=1)\n",
    "        y_arg = torch.argmax(y, dim=1)\n",
    "        for i in range(len(x_arg)):\n",
    "            res[y_arg[i], x_arg[i]] += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b580942b-37cd-445c-b276-3a7c856b8633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  13   0   0 614   0   0 373   0   0]\n",
      " [  0  16   0   0 866   0   0 118   0   0]\n",
      " [  0  14   0   0 696   0   0 290   0   0]\n",
      " [  0  32   0   0 711   0   0 257   0   0]\n",
      " [  0  15   0   0 804   0   0 181   0   0]\n",
      " [  0  28   0   0 647   0   0 325   0   0]\n",
      " [  0  13   0   0 834   0   0 153   0   0]\n",
      " [  0  25   0   0 743   0   0 232   0   0]\n",
      " [  0  18   0   0 790   0   0 192   0   0]\n",
      " [  0  19   0   0 887   0   0  94   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(make_predict_matrix(ntk_model, test_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
