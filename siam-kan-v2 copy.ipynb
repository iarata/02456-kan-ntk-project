{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('/root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional_KANs  __pycache__  efficientKAN.py  kan_convolutional\n",
      "Few-KAN\t\t    data\t env\t\t  wandb\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/AntonioTepsich/Convolutional-KANs\n",
    "# !pip install tqdm pyprof\n",
    "# !mv 'Convolutional-KANs' Convolutional_KANs\n",
    "# !cd Convolutional_KANs && mv kan_convolutional .. \n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# export CUDA_VISIBLE_DEVICES=0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"siam-kan.ipynb\"\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "import torchvision as tv\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from typing import Optional, Tuple, List, Dict, defaultdict\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "\n",
    "\n",
    "from efficientKAN import KAN as EfficientKAN\n",
    "from kan_convolutional.KANConv import KAN_Convolutional_Layer as KANConv\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "seed_val = 42\n",
    "L.seed_everything(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KANDataModule(L.LightningDataModule):\n",
    "    \"\"\"\n",
    "    A PyTorch Lightning DataModule for handling CIFAR10 and MNIST datasets.\n",
    "    Provides unified interface and preprocessing for both datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = \"data\",\n",
    "        dataset_name: str = \"cifar10\",\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 4,\n",
    "        val_split: float = 0.2,\n",
    "        random_seed: int = 42,\n",
    "        img_size: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset_name = dataset_name.lower()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.val_split = val_split\n",
    "        self.random_seed = random_seed\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        if self.dataset_name not in [\"cifar10\", \"mnist\"]:\n",
    "            raise ValueError(\"dataset_name must be either 'cifar10' or 'mnist'\")\n",
    "        \n",
    "        self.num_classes = 10\n",
    "        self.channels = 3 if self.dataset_name == \"cifar10\" else 1\n",
    "        \n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "    def _get_transforms(self) -> Tuple[transforms.Compose, transforms.Compose]:\n",
    "        \"\"\"\n",
    "        Returns train and test transforms for the selected dataset.\n",
    "        Train transforms include augmentations, test transforms only include normalization.\n",
    "        \"\"\"\n",
    "        if self.dataset_name == \"cifar10\":\n",
    "            mean = [0.4914, 0.4822, 0.4465]\n",
    "            std = [0.2470, 0.2435, 0.2616]\n",
    "            \n",
    "            train_transforms = transforms.Compose([\n",
    "                transforms.Resize(self.img_size, interpolation=InterpolationMode.BILINEAR),\n",
    "                transforms.RandomCrop(self.img_size, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ])\n",
    "            \n",
    "            test_transforms = transforms.Compose([\n",
    "                transforms.Resize(self.img_size, interpolation=InterpolationMode.BILINEAR),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ])\n",
    "        \n",
    "        else:\n",
    "            mean = [0.1307]\n",
    "            std = [0.3081]\n",
    "            \n",
    "            train_transforms = transforms.Compose([\n",
    "                transforms.Resize(self.img_size, interpolation=InterpolationMode.BILINEAR),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ])\n",
    "            \n",
    "            test_transforms = transforms.Compose([\n",
    "                transforms.Resize(self.img_size, interpolation=InterpolationMode.BILINEAR),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ])\n",
    "        \n",
    "        return train_transforms, test_transforms\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Downloads the dataset if not already present.\n",
    "        \"\"\"\n",
    "        if self.dataset_name == \"cifar10\":\n",
    "            datasets.CIFAR10(self.data_dir, train=True, download=True)\n",
    "            datasets.CIFAR10(self.data_dir, train=False, download=True)\n",
    "        else:\n",
    "            datasets.MNIST(self.data_dir, train=True, download=True)\n",
    "            datasets.MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Sets up train, validation, and test datasets.\n",
    "        \"\"\"\n",
    "        train_transforms, test_transforms = self._get_transforms()\n",
    "        \n",
    "        if stage == \"fit\" or stage is None:\n",
    "            if self.dataset_name == \"cifar10\":\n",
    "                full_dataset = datasets.CIFAR10(\n",
    "                    self.data_dir, train=True, transform=train_transforms\n",
    "                )\n",
    "            else:\n",
    "                full_dataset = datasets.MNIST(\n",
    "                    self.data_dir, train=True, transform=train_transforms\n",
    "                )\n",
    "            \n",
    "            val_length = int(len(full_dataset) * self.val_split)\n",
    "            train_length = len(full_dataset) - val_length\n",
    "            \n",
    "            self.train_dataset, self.val_dataset = random_split(\n",
    "                full_dataset,\n",
    "                [train_length, val_length],\n",
    "                generator=torch.Generator().manual_seed(self.random_seed)\n",
    "            )\n",
    "            \n",
    "            self.val_dataset.dataset.transform = test_transforms\n",
    "        \n",
    "        if stage == \"test\" or stage is None:\n",
    "            if self.dataset_name == \"cifar10\":\n",
    "                self.test_dataset = datasets.CIFAR10(\n",
    "                    self.data_dir, train=False, transform=test_transforms\n",
    "                )\n",
    "            else:\n",
    "                self.test_dataset = datasets.MNIST(\n",
    "                    self.data_dir, train=False, transform=test_transforms\n",
    "                )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class SiameseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates pairs of images for Siamese network training.\n",
    "    Generates both positive pairs (same class) and negative pairs (different classes).\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, images_per_class: int = 10, n_pairs_per_class: int = 100):\n",
    "        self.dataset = dataset\n",
    "        self.images_per_class = images_per_class\n",
    "        self.n_pairs_per_class = n_pairs_per_class\n",
    "        \n",
    "        all_class_indices: Dict[int, List[int]] = defaultdict(list)\n",
    "        for idx, (_, label) in enumerate(dataset):\n",
    "            all_class_indices[label].append(idx)\n",
    "        \n",
    "        self.class_indices: Dict[int, List[int]] = {}\n",
    "        for label, indices in all_class_indices.items():\n",
    "            if len(indices) >= images_per_class:\n",
    "                self.class_indices[label] = random.sample(indices, images_per_class)\n",
    "        \n",
    "        self.pairs = self._generate_pairs()\n",
    "    \n",
    "    def _generate_pairs(self):\n",
    "        pairs = []\n",
    "        for label in self.class_indices:\n",
    "            indices = self.class_indices[label]\n",
    "            \n",
    "            for i in range(len(indices)):\n",
    "                for j in range(i + 1, len(indices)):\n",
    "                    pairs.append((indices[i], indices[j], 1)) \n",
    "        \n",
    "        n_neg_pairs = len(pairs) \n",
    "        for _ in range(n_neg_pairs):\n",
    "            label1, label2 = random.sample(list(self.class_indices.keys()), 2)\n",
    "            idx1 = random.choice(self.class_indices[label1])\n",
    "            idx2 = random.choice(self.class_indices[label2])\n",
    "            pairs.append((idx1, idx2, 0))\n",
    "        \n",
    "        random.shuffle(pairs)\n",
    "        return pairs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx1, idx2, label = self.pairs[idx]\n",
    "        img1, class1 = self.dataset[idx1]\n",
    "        img2, class2 = self.dataset[idx2]\n",
    "        return img1, img2, torch.tensor(label, dtype=torch.float32), class1, class2\n",
    "\n",
    "class SiameseDataModule(L.LightningDataModule):\n",
    "    \"\"\"\n",
    "    Data module for Siamese network training with CIFAR10 or MNIST datasets.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = \"data\",\n",
    "        dataset_name: str = \"cifar10\",\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 4,\n",
    "        images_per_class: int = 10,\n",
    "        n_pairs_per_class: int = 100,\n",
    "        val_split: float = 0.2,\n",
    "        img_size: int = 32,\n",
    "        random_seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset_name = dataset_name.lower()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.images_per_class = images_per_class\n",
    "        self.n_pairs_per_class = n_pairs_per_class\n",
    "        self.val_split = val_split\n",
    "        self.img_size = img_size\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        if self.dataset_name not in [\"cifar10\", \"mnist\"]:\n",
    "            raise ValueError(\"dataset_name must be either 'cifar10' or 'mnist'\")\n",
    "        \n",
    "        self.num_classes = 10\n",
    "        self.channels = 3 if self.dataset_name == \"cifar10\" else 1\n",
    "    \n",
    "    def _get_transforms(self):\n",
    "        if self.dataset_name == \"cifar10\":\n",
    "            mean = [0.4914, 0.4822, 0.4465]\n",
    "            std = [0.2470, 0.2435, 0.2616]\n",
    "        else:\n",
    "            mean = [0.1307]\n",
    "            std = [0.3081]\n",
    "        \n",
    "        train_transforms = transforms.Compose([\n",
    "            transforms.Resize(self.img_size, interpolation=InterpolationMode.BILINEAR),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "        \n",
    "        eval_transforms = transforms.Compose([\n",
    "            transforms.Resize(self.img_size, interpolation=InterpolationMode.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "        \n",
    "        return train_transforms, eval_transforms\n",
    "\n",
    "    def prepare_data(self):\n",
    "        if self.dataset_name == \"cifar10\":\n",
    "            datasets.CIFAR10(self.data_dir, train=True, download=True)\n",
    "            datasets.CIFAR10(self.data_dir, train=False, download=True)\n",
    "        else:\n",
    "            datasets.MNIST(self.data_dir, train=True, download=True)\n",
    "            datasets.MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        train_transforms, eval_transforms = self._get_transforms()\n",
    "        \n",
    "        if stage == \"fit\" or stage is None:\n",
    "            if self.dataset_name == \"cifar10\":\n",
    "                full_dataset = datasets.CIFAR10(\n",
    "                    self.data_dir, train=True, transform=train_transforms\n",
    "                )\n",
    "            else:\n",
    "                full_dataset = datasets.MNIST(\n",
    "                    self.data_dir, train=True, transform=train_transforms\n",
    "                )\n",
    "            \n",
    "            dataset_size = len(full_dataset)\n",
    "            val_size = int(dataset_size * self.val_split)\n",
    "            train_size = dataset_size - val_size\n",
    "            \n",
    "            train_subset, val_subset = torch.utils.data.random_split(\n",
    "                full_dataset,\n",
    "                [train_size, val_size],\n",
    "                generator=torch.Generator().manual_seed(self.random_seed)\n",
    "            )\n",
    "            \n",
    "            self.train_dataset = SiameseDataset(\n",
    "                train_subset,\n",
    "                images_per_class=self.images_per_class,\n",
    "                n_pairs_per_class=self.n_pairs_per_class\n",
    "            )\n",
    "            \n",
    "            self.val_dataset = SiameseDataset(\n",
    "                val_subset,\n",
    "                images_per_class=self.images_per_class,\n",
    "                n_pairs_per_class=self.n_pairs_per_class // 2 \n",
    "            )\n",
    "        \n",
    "        if stage == \"test\" or stage is None:\n",
    "            if self.dataset_name == \"cifar10\":\n",
    "                self.test_dataset = datasets.CIFAR10(\n",
    "                    self.data_dir, train=False, transform=eval_transforms\n",
    "                )\n",
    "            else:\n",
    "                self.test_dataset = datasets.MNIST(\n",
    "                    self.data_dir, train=False, transform=eval_transforms\n",
    "                )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KANModule(L.LightningModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning module for KAN Networks with integrated W&B logging\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"kan_basic\",\n",
    "        num_classes: int = 10,\n",
    "        learning_rate: float = 1e-3,\n",
    "        weight_decay: float = 1e-5,\n",
    "        channels: int = 3,\n",
    "        img_size: int = 32,\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 3,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = self._create_model()\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "        self.val_predictions = []\n",
    "        self.val_targets = []\n",
    "\n",
    "    def _create_model(self) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Create the specified KAN architecture\n",
    "        \"\"\"\n",
    "        input_dim = self.hparams.channels * self.hparams.img_size * self.hparams.img_size\n",
    "        \n",
    "        if self.hparams.model_name == \"kan_basic\":\n",
    "            return KANBasic(\n",
    "                input_dim=input_dim,\n",
    "                hidden_dim=self.hparams.hidden_dim,\n",
    "                num_classes=self.hparams.num_classes,\n",
    "                dropout=self.hparams.dropout\n",
    "            )\n",
    "        elif self.hparams.model_name == \"kan_with_CNN\":\n",
    "            return KANwithCNN(\n",
    "                input_dim=input_dim,\n",
    "                hidden_dim=self.hparams.hidden_dim,\n",
    "                num_classes=self.hparams.num_classes,\n",
    "                num_layers=self.hparams.num_layers,\n",
    "                dropout=self.hparams.dropout\n",
    "            )\n",
    "        elif self.hparams.model_name == \"kkan\":\n",
    "            return KKan(\n",
    "                input_dim=input_dim,\n",
    "                hidden_dim=self.hparams.hidden_dim,\n",
    "                num_classes=self.hparams.num_classes,\n",
    "                num_layers=self.hparams.num_layers,\n",
    "                dropout=self.hparams.dropout\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model name: {self.hparams.model_name}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.1,\n",
    "            patience=5,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        self.train_acc(preds, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", self.train_acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        self.val_predictions.extend(preds.cpu().numpy())\n",
    "        self.val_targets.extend(y.cpu().numpy())\n",
    "        \n",
    "        self.val_acc(preds, y)\n",
    "        \n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Create and log visualizations to W&B at the end of validation\n",
    "        \"\"\"\n",
    "        y_pred = np.array(self.val_predictions)\n",
    "        y_true = np.array(self.val_targets)\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix - Epoch {self.current_epoch}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        \n",
    "        \n",
    "        wandb.log({\n",
    "            \"confusion_matrix\": wandb.Image(plt),\n",
    "            \"epoch\": self.current_epoch\n",
    "        })\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(y_pred, bins=self.hparams.num_classes, alpha=0.5, label='Predictions')\n",
    "        plt.hist(y_true, bins=self.hparams.num_classes, alpha=0.5, label='Ground Truth')\n",
    "        plt.title(f'Prediction Distribution - Epoch {self.current_epoch}')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.legend()\n",
    "        \n",
    "        \n",
    "        wandb.log({\n",
    "            \"prediction_distribution\": wandb.Image(plt),\n",
    "            \"epoch\": self.current_epoch\n",
    "        })\n",
    "        \n",
    "        # Clear stored predictions\n",
    "        self.val_predictions = []\n",
    "        self.val_targets = []\n",
    "        \n",
    "        plt.close('all')\n",
    "\n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        self.test_acc(preds, y)\n",
    "        \n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_acc\", self.test_acc)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# KAN Model Architectures\n",
    "class SampleCNN(nn.Module):\n",
    "    def __init__(self, in_channels: int, hidden_dim: int, num_classes: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # simple CNN architecture\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten() \n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "class KANBasic(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            EfficientKAN([input_dim, input_dim//2, input_dim//4, 64, num_classes]),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "class KANwithCNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int, num_layers: int = 3, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.img_size = int(np.sqrt(input_dim))\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.conv_output_dim = 64 * (self.img_size // 4) * (self.img_size // 4)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Linear(self.conv_output_dim, hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(hidden_dim, num_classes)\n",
    "            nn.Dropout(dropout),\n",
    "            EfficientKAN([self.conv_output_dim, self.conv_output_dim//2, self.conv_output_dim//4, 64, num_classes]),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Reshape flattened input to image format\n",
    "        x = x.view(-1, 1, self.img_size, self.img_size)\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class KKan(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int, num_layers: int = 3, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.img_size = int(np.sqrt(input_dim)) \n",
    "        \n",
    "        self.conv1 = KANConv(\n",
    "            in_channels=1, \n",
    "            out_channels=6,\n",
    "            kernel_size=(3,3),\n",
    "        )\n",
    "\n",
    "        self.conv2 = KANConv(\n",
    "            in_channels=6,\n",
    "            out_channels=12,\n",
    "            kernel_size=(3,3),\n",
    "        )\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        conv_output_size = ((self.img_size - 2) // 2 - 2) // 2\n",
    "        self.linear1 = nn.Linear(12 * conv_output_size * conv_output_size, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 1, self.img_size, self.img_size)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.flat(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "    #     self.img_size = int(np.sqrt(input_dim // 3))\n",
    "        \n",
    "    #     self.conv1 = KANConv(\n",
    "    #         in_channels=3,  # Changed from 1 to 3 for RGB\n",
    "    #         out_channels=5,\n",
    "    #         kernel_size=(3,3),\n",
    "    #     )\n",
    "\n",
    "    #     self.conv2 = KANConv(\n",
    "    #         in_channels=5,\n",
    "    #         out_channels=5,\n",
    "    #         kernel_size=(3,3),\n",
    "    #     )\n",
    "\n",
    "    #     self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "    #     self.flat = nn.Flatten()\n",
    "        \n",
    "    #     # Calculate the size after convolutions and pooling\n",
    "    #     conv_output_size = ((self.img_size // 2) // 2)  # After two pooling layers\n",
    "    #     self.linear1 = nn.Linear(180, num_classes)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # Reshape the flattened input back to image format\n",
    "    #     x = x.view(x.size(0), 3, self.img_size, self.img_size)\n",
    "        \n",
    "    #     x = self.conv1(x)\n",
    "    #     x = self.pool1(x)\n",
    "        \n",
    "    #     x = self.conv2(x)\n",
    "    #     x = self.pool1(x)\n",
    "        \n",
    "    #     x = self.flat(x)\n",
    "    #     x = self.linear1(x)\n",
    "    #     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, distance, label):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            distance: euclidean distance between pairs\n",
    "            label: 1 for similar pairs, 0 for dissimilar pairs\n",
    "        \"\"\"\n",
    "        loss_contrastive = torch.mean(\n",
    "            label * torch.pow(distance, 2) + \n",
    "            (1 - label) * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2) \n",
    "        )\n",
    "        return loss_contrastive\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Siamese network implementation with shared encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, embedding_dim: int = 128, channels: int = 1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.channels = channels\n",
    "        \n",
    "        self.flat_dim = input_dim * channels\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.flat_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, embedding_dim)\n",
    "        )\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.forward_one(x1)\n",
    "        out2 = self.forward_one(x2)\n",
    "        return out1, out2\n",
    "\n",
    "class SiameseModule(L.LightningModule):\n",
    "    \"\"\"\n",
    "    Lightning module for training Siamese network with contrastive loss.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 32,\n",
    "        channels: int = 1,\n",
    "        hidden_dim: int = 512,\n",
    "        embedding_dim: int = 128,\n",
    "        learning_rate: float = 1e-3,\n",
    "        margin: float = 2.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        input_dim = img_size * img_size\n",
    "        \n",
    "        self.model = SiameseNetwork(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            channels=channels\n",
    "        )\n",
    "        \n",
    "        self.criterion = ContrastiveLoss(margin=margin)\n",
    "        \n",
    "        self.train_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "        self.test_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "        self.test_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=10)\n",
    "        self.test_data = None\n",
    "        self.test_embeddings = None\n",
    "        self.test_labels = None\n",
    "        \n",
    "        self.class_prototypes = {}\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        return self.model(x1, x2)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.1,\n",
    "            patience=5,\n",
    "            verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "            },\n",
    "        }\n",
    "    \n",
    "    def compute_distance_and_prediction(self, emb1, emb2):\n",
    "        \"\"\"Compute euclidean distance and binary prediction\"\"\"\n",
    "        distance = F.pairwise_distance(emb1, emb2)\n",
    "        predictions = (distance < self.hparams.margin/2).float()\n",
    "        return distance, predictions\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img1, img2, label, _, _ = batch\n",
    "        emb1, emb2 = self(img1, img2)\n",
    "        distance, predictions = self.compute_distance_and_prediction(emb1, emb2)\n",
    "        loss = self.criterion(distance, label)\n",
    "        \n",
    "        self.train_accuracy(predictions, label)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", self.train_accuracy, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img1, img2, label, _, _ = batch\n",
    "        emb1, emb2 = self(img1, img2)\n",
    "        distance, predictions = self.compute_distance_and_prediction(emb1, emb2)\n",
    "        loss = self.criterion(distance, label)\n",
    "        \n",
    "        self.val_accuracy(predictions, label)\n",
    "        \n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Store batch data for processing in test_epoch_end.\n",
    "        We need all test data to perform query-based evaluation.\n",
    "        \"\"\"\n",
    "        images, labels = batch\n",
    "        \n",
    "        embeddings = self.model.forward_one(images)\n",
    "        \n",
    "        if self.test_data is None:\n",
    "            self.test_data = images\n",
    "            self.test_embeddings = embeddings\n",
    "            self.test_labels = labels\n",
    "        else:\n",
    "            self.test_data = torch.cat([self.test_data, images])\n",
    "            self.test_embeddings = torch.cat([self.test_embeddings, embeddings])\n",
    "            self.test_labels = torch.cat([self.test_labels, labels])\n",
    "    \n",
    "    def on_test_epoch_start(self):\n",
    "        \"\"\"Reset stored test data at the start of test epoch\"\"\"\n",
    "        self.test_data = None\n",
    "        self.test_embeddings = None\n",
    "        self.test_labels = None\n",
    "        \n",
    "    def on_test_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Perform query-based evaluation at the end of test epoch\n",
    "        \"\"\"\n",
    "        class_indices = defaultdict(list)\n",
    "        for idx, label in enumerate(self.test_labels):\n",
    "            class_indices[label.item()].append(idx)\n",
    "        \n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "        per_class_accuracy = defaultdict(list)\n",
    "        \n",
    "        for query_class in class_indices.keys():\n",
    "            query_idx = random.choice(class_indices[query_class])\n",
    "            query_embedding = self.test_embeddings[query_idx]\n",
    "            \n",
    "            distances = F.pairwise_distance(\n",
    "                query_embedding.unsqueeze(0).repeat(len(self.test_embeddings), 1),\n",
    "                self.test_embeddings\n",
    "            )\n",
    "            \n",
    "            class_distances = defaultdict(list)\n",
    "            for idx, dist in enumerate(distances):\n",
    "                class_distances[self.test_labels[idx].item()].append(dist.item())\n",
    "            \n",
    "            avg_class_distances = {\n",
    "                cls: np.mean(dists) for cls, dists in class_distances.items()\n",
    "            }\n",
    "            \n",
    "            predicted_class = min(avg_class_distances.items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "            all_predictions.append(predicted_class)\n",
    "            all_true_labels.append(query_class)\n",
    "            \n",
    "            is_correct = predicted_class == query_class\n",
    "            per_class_accuracy[query_class].append(is_correct)\n",
    "            \n",
    "            self.log(f\"test_query_class_{query_class}_predicted\", predicted_class)\n",
    "            self.log(f\"test_query_class_{query_class}_distance\", avg_class_distances[predicted_class])\n",
    "        \n",
    "        correct_predictions = sum(p == t for p, t in zip(all_predictions, all_true_labels))\n",
    "        overall_accuracy = correct_predictions / len(all_predictions)\n",
    "        \n",
    "        class_accuracies = {\n",
    "            cls: sum(results) / len(results) \n",
    "            for cls, results in per_class_accuracy.items()\n",
    "        }\n",
    "        \n",
    "        self.log(\"test_accuracy\", overall_accuracy)\n",
    "        for cls, acc in class_accuracies.items():\n",
    "            self.log(f\"test_class_{cls}_accuracy\", acc)\n",
    "        \n",
    "        cm = confusion_matrix(all_true_labels, all_predictions)\n",
    "        \n",
    "        print(\"\\nTest Results:\")\n",
    "        print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "        print(\"\\nPer-class Accuracy:\")\n",
    "        for cls, acc in class_accuracies.items():\n",
    "            print(f\"Class {cls}: {acc:.4f}\")\n",
    "        \n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        if hasattr(self.logger, 'experiment') and hasattr(self.logger.experiment, 'log'):\n",
    "            try:\n",
    "                fig, ax = plt.subplots(figsize=(10, 8))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', ax=ax)\n",
    "                plt.title(\"Test Confusion Matrix\")\n",
    "                plt.xlabel(\"Predicted Class\")\n",
    "                plt.ylabel(\"True Class\")\n",
    "                \n",
    "                self.logger.experiment.log({\n",
    "                    \"test_confusion_matrix\": wandb.Image(fig),\n",
    "                    \"test_epoch\": self.current_epoch\n",
    "                })\n",
    "                plt.close()\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    def predict_class(self, query_image):\n",
    "        \"\"\"Predict class for a query image using class prototypes\"\"\"\n",
    "        query_embedding = self.model.forward_one(query_image.unsqueeze(0))\n",
    "        \n",
    "        distances = {}\n",
    "        for class_idx, prototype in self.class_prototypes.items():\n",
    "            distance = F.pairwise_distance(\n",
    "                query_embedding,\n",
    "                prototype.unsqueeze(0)\n",
    "            )\n",
    "            distances[class_idx] = distance.item()\n",
    "        \n",
    "        return min(distances.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "def predict_query_similarity(self, query_image, reference_images):\n",
    "    \"\"\"\n",
    "    Utility method to compute similarities between a query image and reference images\n",
    "    \"\"\"\n",
    "    query_embedding = self.model.forward_one(query_image.unsqueeze(0))\n",
    "    reference_embeddings = self.model.forward_one(reference_images)\n",
    "    \n",
    "    distances = F.pairwise_distance(\n",
    "        query_embedding.repeat(len(reference_embeddings), 1),\n",
    "        reference_embeddings\n",
    "    )\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseKANNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Siamese network implementation that can use different KAN architectures as encoders.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        embedding_dim: int = 128,\n",
    "        channels: int = 1,\n",
    "        architecture: str = \"kan_basic\"  # kan_basic, kan_with_cnn, kkan\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.channels = channels\n",
    "        self.architecture = architecture\n",
    "        self.img_size = int(np.sqrt(input_dim))\n",
    "        \n",
    "        if architecture == \"kan_basic\":\n",
    "            self.encoder = KANBasicEncoder(\n",
    "                input_dim=input_dim * channels,\n",
    "                hidden_dim=hidden_dim,\n",
    "                embedding_dim=embedding_dim\n",
    "            )\n",
    "        elif architecture == \"kan_with_cnn\":\n",
    "            self.encoder = KANwithCNNEncoder(\n",
    "                img_size=self.img_size,\n",
    "                channels=channels,\n",
    "                hidden_dim=hidden_dim,\n",
    "                embedding_dim=embedding_dim\n",
    "            )\n",
    "        elif architecture == \"kkan\":\n",
    "            self.encoder = KKANEncoder(\n",
    "                img_size=self.img_size,\n",
    "                channels=channels,\n",
    "                embedding_dim=embedding_dim\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown architecture: {architecture}\")\n",
    "    \n",
    "    def forward_one(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.forward_one(x1)\n",
    "        out2 = self.forward_one(x2)\n",
    "        return out1, out2\n",
    "\n",
    "class KANBasicEncoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            EfficientKAN([input_dim, hidden_dim, hidden_dim//2, hidden_dim//4, embedding_dim]),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x.view(x.size(0), -1))\n",
    "\n",
    "class KANwithCNNEncoder(nn.Module):\n",
    "    def __init__(self, img_size: int, channels: int, hidden_dim: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(channels, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        conv_output_dim = 64 * (img_size // 8) * (img_size // 8)\n",
    "        \n",
    "        kan_dims = [\n",
    "            conv_output_dim,\n",
    "            conv_output_dim // 2,\n",
    "            conv_output_dim // 4,\n",
    "            hidden_dim,\n",
    "            embedding_dim\n",
    "        ]\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            EfficientKAN(kan_dims),\n",
    "            nn.BatchNorm1d(embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class KKANEncoder(nn.Module):\n",
    "    def __init__(self, img_size: int, channels: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = KANConv(\n",
    "            in_channels=channels,\n",
    "            out_channels=6,\n",
    "            kernel_size=(3,3),\n",
    "        )\n",
    "\n",
    "        self.conv2 = KANConv(\n",
    "            in_channels=6,\n",
    "            out_channels=12,\n",
    "            kernel_size=(3,3),\n",
    "        )\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        conv_output_size = ((img_size - 2) // 2 - 2) // 2\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(12 * conv_output_size * conv_output_size, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flat(x)\n",
    "        return self.final(x)\n",
    "\n",
    "class SiameseKANModule(L.LightningModule):\n",
    "    \"\"\"\n",
    "    Lightning module for Siamese network with KAN architectures\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 32,\n",
    "        channels: int = 1,\n",
    "        hidden_dim: int = 512,\n",
    "        embedding_dim: int = 128,\n",
    "        learning_rate: float = 1e-3,\n",
    "        margin: float = 2.0,\n",
    "        architecture: str = \"kan_basic\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        input_dim = img_size * img_size\n",
    "        \n",
    "        self.model = SiameseKANNetwork(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            channels=channels,\n",
    "            architecture=architecture\n",
    "        )\n",
    "        \n",
    "        self.criterion = ContrastiveLoss(margin=margin)\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "        self.test_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "        \n",
    "        self.test_data = None\n",
    "        self.test_embeddings = None\n",
    "        self.test_labels = None\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.1,\n",
    "            patience=5,\n",
    "            verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "            },\n",
    "        }\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        return self.model(x1, x2)\n",
    "    \n",
    "    def compute_distance_and_prediction(self, emb1, emb2):\n",
    "        distance = F.pairwise_distance(emb1, emb2)\n",
    "        predictions = (distance < self.hparams.margin/2).float()\n",
    "        return distance, predictions\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img1, img2, label, _, _ = batch\n",
    "        emb1, emb2 = self(img1, img2)\n",
    "        distance, predictions = self.compute_distance_and_prediction(emb1, emb2)\n",
    "        loss = self.criterion(distance, label)\n",
    "        \n",
    "        self.train_accuracy(predictions, label)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", self.train_accuracy, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img1, img2, label, _, _ = batch\n",
    "        emb1, emb2 = self(img1, img2)\n",
    "        distance, predictions = self.compute_distance_and_prediction(emb1, emb2)\n",
    "        loss = self.criterion(distance, label)\n",
    "        \n",
    "        self.val_accuracy(predictions, label)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Store batch data for processing in test_epoch_end.\n",
    "        We need all test data to perform query-based evaluation.\n",
    "        \"\"\"\n",
    "        images, labels = batch\n",
    "        \n",
    "        embeddings = self.model.forward_one(images)\n",
    "        \n",
    "        if self.test_data is None:\n",
    "            self.test_data = images\n",
    "            self.test_embeddings = embeddings\n",
    "            self.test_labels = labels\n",
    "        else:\n",
    "            self.test_data = torch.cat([self.test_data, images])\n",
    "            self.test_embeddings = torch.cat([self.test_embeddings, embeddings])\n",
    "            self.test_labels = torch.cat([self.test_labels, labels])\n",
    "    \n",
    "    def on_test_epoch_start(self):\n",
    "        \"\"\"Reset stored test data at the start of test epoch\"\"\"\n",
    "        self.test_data = None\n",
    "        self.test_embeddings = None\n",
    "        self.test_labels = None\n",
    "        \n",
    "    def on_test_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Perform query-based evaluation at the end of test epoch\n",
    "        \"\"\"\n",
    "        class_indices = defaultdict(list)\n",
    "        for idx, label in enumerate(self.test_labels):\n",
    "            class_indices[label.item()].append(idx)\n",
    "        \n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "        per_class_accuracy = defaultdict(list)\n",
    "        \n",
    "        for query_class in class_indices.keys():\n",
    "            query_idx = random.choice(class_indices[query_class])\n",
    "            query_embedding = self.test_embeddings[query_idx]\n",
    "            \n",
    "            distances = F.pairwise_distance(\n",
    "                query_embedding.unsqueeze(0).repeat(len(self.test_embeddings), 1),\n",
    "                self.test_embeddings\n",
    "            )\n",
    "            \n",
    "            class_distances = defaultdict(list)\n",
    "            for idx, dist in enumerate(distances):\n",
    "                class_distances[self.test_labels[idx].item()].append(dist.item())\n",
    "            \n",
    "            avg_class_distances = {\n",
    "                cls: np.mean(dists) for cls, dists in class_distances.items()\n",
    "            }\n",
    "            \n",
    "            predicted_class = min(avg_class_distances.items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "            all_predictions.append(predicted_class)\n",
    "            all_true_labels.append(query_class)\n",
    "            \n",
    "            is_correct = predicted_class == query_class\n",
    "            per_class_accuracy[query_class].append(is_correct)\n",
    "            \n",
    "            self.log(f\"test_query_class_{query_class}_predicted\", predicted_class)\n",
    "            self.log(f\"test_query_class_{query_class}_distance\", avg_class_distances[predicted_class])\n",
    "        \n",
    "        correct_predictions = sum(p == t for p, t in zip(all_predictions, all_true_labels))\n",
    "        overall_accuracy = correct_predictions / len(all_predictions)\n",
    "        \n",
    "        class_accuracies = {\n",
    "            cls: sum(results) / len(results) \n",
    "            for cls, results in per_class_accuracy.items()\n",
    "        }\n",
    "        \n",
    "        self.log(\"test_accuracy\", overall_accuracy)\n",
    "        for cls, acc in class_accuracies.items():\n",
    "            self.log(f\"test_class_{cls}_accuracy\", acc)\n",
    "        \n",
    "        cm = confusion_matrix(all_true_labels, all_predictions)\n",
    "        \n",
    "        print(\"\\nTest Results:\")\n",
    "        print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "        print(\"\\nPer-class Accuracy:\")\n",
    "        for cls, acc in class_accuracies.items():\n",
    "            print(f\"Class {cls}: {acc:.4f}\")\n",
    "        \n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        if hasattr(self.logger, 'experiment') and hasattr(self.logger.experiment, 'log'):\n",
    "            try:\n",
    "                fig, ax = plt.subplots(figsize=(10, 8))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', ax=ax)\n",
    "                plt.title(\"Test Confusion Matrix\")\n",
    "                plt.xlabel(\"Predicted Class\")\n",
    "                plt.ylabel(\"True Class\")\n",
    "                \n",
    "                self.logger.experiment.log({\n",
    "                    \"test_confusion_matrix\": wandb.Image(fig),\n",
    "                    \"test_epoch\": self.current_epoch\n",
    "                })\n",
    "                plt.close()\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    def predict_class(self, query_image):\n",
    "        \"\"\"Predict class for a query image using class prototypes\"\"\"\n",
    "        query_embedding = self.model.forward_one(query_image.unsqueeze(0))\n",
    "        \n",
    "        distances = {}\n",
    "        for class_idx, prototype in self.class_prototypes.items():\n",
    "            distance = F.pairwise_distance(\n",
    "                query_embedding,\n",
    "                prototype.unsqueeze(0)\n",
    "            )\n",
    "            distances[class_idx] = distance.item()\n",
    "        \n",
    "        return min(distances.items(), key=lambda x: x[1])[0]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training siam.mnist.kan_with_cnn.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miarata\u001b[0m (\u001b[33mhdu-dk\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20241209_100251-ph24xye6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hdu-dk/Few-KAN/runs/ph24xye6' target=\"_blank\">siam.mnist.kan_with_cnn.15</a></strong> to <a href='https://wandb.ai/hdu-dk/Few-KAN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hdu-dk/Few-KAN' target=\"_blank\">https://wandb.ai/hdu-dk/Few-KAN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hdu-dk/Few-KAN/runs/ph24xye6' target=\"_blank\">https://wandb.ai/hdu-dk/Few-KAN/runs/ph24xye6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/dtu/blackhole/03/213187/kan/env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name           | Type              | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model          | SiameseKANNetwork | 3.5 M  | train\n",
      "1 | criterion      | ContrastiveLoss   | 0      | train\n",
      "2 | train_accuracy | BinaryAccuracy    | 0      | train\n",
      "3 | val_accuracy   | BinaryAccuracy    | 0      | train\n",
      "4 | test_accuracy  | BinaryAccuracy    | 0      | train\n",
      "-------------------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.5 M     Total params\n",
      "13.959    Total estimated model params size (MB)\n",
      "30        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce4ff41080c43f880ee8f5b03d3421d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                          | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7cb4ac6e364fca802318a6e607658f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                 | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168d09bd25284187aed6c22360aabc4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599f78bb04044fe5b3d8c157cc1c7823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32f3dab118147e08f66a649ac4364ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f96f4be3707479abf3642b459485584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd3a7c7b0ea482888b5bd8e297324fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45fd75d695445b48d960136d6dc8575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81658c8ee7214f46b443e1c585a53faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c388e8ae274c189d2713b2e04427bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a76499543664e0f853d4e9f76eaaf54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cad3ac8df3447a8154e4d40aa30024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bcf942241884c2891e8cac2be87f84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1464090522f40d2899b9b1b64e73235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3196713d742c4cf599d31c50ff847ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13208d6b253e4b82b5141b201ca0e091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc8ec50747b4c18bf627b488c7875ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5393397974704e298715a1eab573d131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1314a10bc0fd42e5ac3a46e5c0bdb90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f68d438a0454d76a51bc49fbc576d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb8e049d0c04dc5b776fefd73e13310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e0b509b4854288b7e012006312e194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e06d461d1c244298f1bd73e066b58df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487d9158bec04d10875f3f93e2a29576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295274fac973490db2e638b27e4ab615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b13c35d91254f8fab0f7f2b08d0d24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87c4eb9b14a42f7a713139052fa352e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2468307ac14d6b97aabe9028e6fd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc4fb6b69e14d47bdc565a5905f8039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1fd64614e24a8f8776f3a623295f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97605c8bfdaa4654a6b9ae4ae1d4cb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bb19683c7d48cd979cfa5104abd15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bf4979d0c44e77a1315ca384c24f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ac1e697c7a4331acd16aa4b5e84b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6730409fcc430d99d2cfc351b5d988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb25c21dee24f36bcbeab29ee720d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec7509327344b77aa0be8c9a35c7a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df37de66fde4f3a8907c5eab20dee84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5fc3a75c51435495175a925dabebeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31a3ad5ac964731abed056868b7bc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1911abf05347b78efb6d6d55c78a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862c17987d884401b9e967386cc52879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c008b1a2bf014e09b249cf11c8e88d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e096423bc7947e8968ea1df4bb613a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709206e1df8d4ff3b2c9d4709082b94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cab5191a6d433891951b135c23b39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de9e2a1b3994d668707bc16b1663d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                               | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23eb3ff658f4def9f868aa334c0aa1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                                  | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "Overall Accuracy: 0.6000\n",
      "\n",
      "Per-class Accuracy:\n",
      "Class 7: 1.0000\n",
      "Class 2: 0.0000\n",
      "Class 1: 1.0000\n",
      "Class 0: 1.0000\n",
      "Class 4: 1.0000\n",
      "Class 9: 1.0000\n",
      "Class 5: 0.0000\n",
      "Class 6: 1.0000\n",
      "Class 3: 0.0000\n",
      "Class 8: 0.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 1]]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        Test metric                 DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       test_accuracy             0.6000000238418579\n",
      "   test_class_0_accuracy                1.0\n",
      "   test_class_1_accuracy                1.0\n",
      "   test_class_2_accuracy                0.0\n",
      "   test_class_3_accuracy                0.0\n",
      "   test_class_4_accuracy                1.0\n",
      "   test_class_5_accuracy                0.0\n",
      "   test_class_6_accuracy                1.0\n",
      "   test_class_7_accuracy                1.0\n",
      "   test_class_8_accuracy                0.0\n",
      "   test_class_9_accuracy                1.0\n",
      "test_query_class_0_distance     0.49294090270996094\n",
      "test_query_class_0_predicted            0.0\n",
      "test_query_class_1_distance      0.4899050295352936\n",
      "test_query_class_1_predicted            1.0\n",
      "test_query_class_2_distance      0.7908484935760498\n",
      "test_query_class_2_predicted            3.0\n",
      "test_query_class_3_distance      0.7619850635528564\n",
      "test_query_class_3_predicted            9.0\n",
      "test_query_class_4_distance      0.4884156286716461\n",
      "test_query_class_4_predicted            4.0\n",
      "test_query_class_5_distance      0.6259030103683472\n",
      "test_query_class_5_predicted            3.0\n",
      "test_query_class_6_distance      0.5747101306915283\n",
      "test_query_class_6_predicted            6.0\n",
      "test_query_class_7_distance      0.5034176111221313\n",
      "test_query_class_7_predicted            7.0\n",
      "test_query_class_8_distance      0.8573524951934814\n",
      "test_query_class_8_predicted            9.0\n",
      "test_query_class_9_distance      0.5699731707572937\n",
      "test_query_class_9_predicted            9.0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇█████</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_class_0_accuracy</td><td>▁</td></tr><tr><td>test_class_1_accuracy</td><td>▁</td></tr><tr><td>test_class_2_accuracy</td><td>▁</td></tr><tr><td>test_class_3_accuracy</td><td>▁</td></tr><tr><td>test_class_4_accuracy</td><td>▁</td></tr><tr><td>test_class_5_accuracy</td><td>▁</td></tr><tr><td>test_class_6_accuracy</td><td>▁</td></tr><tr><td>test_class_7_accuracy</td><td>▁</td></tr><tr><td>test_class_8_accuracy</td><td>▁</td></tr><tr><td>test_class_9_accuracy</td><td>▁</td></tr><tr><td>test_epoch</td><td>▁</td></tr><tr><td>test_query_class_0_distance</td><td>▁</td></tr><tr><td>test_query_class_0_predicted</td><td>▁</td></tr><tr><td>test_query_class_1_distance</td><td>▁</td></tr><tr><td>test_query_class_1_predicted</td><td>▁</td></tr><tr><td>test_query_class_2_distance</td><td>▁</td></tr><tr><td>test_query_class_2_predicted</td><td>▁</td></tr><tr><td>test_query_class_3_distance</td><td>▁</td></tr><tr><td>test_query_class_3_predicted</td><td>▁</td></tr><tr><td>test_query_class_4_distance</td><td>▁</td></tr><tr><td>test_query_class_4_predicted</td><td>▁</td></tr><tr><td>test_query_class_5_distance</td><td>▁</td></tr><tr><td>test_query_class_5_predicted</td><td>▁</td></tr><tr><td>test_query_class_6_distance</td><td>▁</td></tr><tr><td>test_query_class_6_predicted</td><td>▁</td></tr><tr><td>test_query_class_7_distance</td><td>▁</td></tr><tr><td>test_query_class_7_predicted</td><td>▁</td></tr><tr><td>test_query_class_8_distance</td><td>▁</td></tr><tr><td>test_query_class_8_predicted</td><td>▁</td></tr><tr><td>test_query_class_9_distance</td><td>▁</td></tr><tr><td>test_query_class_9_predicted</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▅▇▅▅▅▆▆▇██████▆█▇▇▇▇▄▇▇▇█▇█▇▇▇█▇█▇██▇█</td></tr><tr><td>train_loss</td><td>▆▆▃▆▅▇█▅▃▃▄▁▃▆▁▁▃▁▄▁▄▂▂▂▁▂▁▂▂▄▃▁▃▃▁▂▁▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇█████</td></tr><tr><td>val_acc</td><td>▁▁▄▃▃▄▅▄▆▇▇▆▇▇▇▇▇▇▆▇▇▇█▇▇▇▇▇████████▇███</td></tr><tr><td>val_loss</td><td>█▇▆▆▆▅▄▅▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>45</td></tr><tr><td>test_accuracy</td><td>0.6</td></tr><tr><td>test_class_0_accuracy</td><td>1</td></tr><tr><td>test_class_1_accuracy</td><td>1</td></tr><tr><td>test_class_2_accuracy</td><td>0</td></tr><tr><td>test_class_3_accuracy</td><td>0</td></tr><tr><td>test_class_4_accuracy</td><td>1</td></tr><tr><td>test_class_5_accuracy</td><td>0</td></tr><tr><td>test_class_6_accuracy</td><td>1</td></tr><tr><td>test_class_7_accuracy</td><td>1</td></tr><tr><td>test_class_8_accuracy</td><td>0</td></tr><tr><td>test_class_9_accuracy</td><td>1</td></tr><tr><td>test_epoch</td><td>45</td></tr><tr><td>test_query_class_0_distance</td><td>0.49294</td></tr><tr><td>test_query_class_0_predicted</td><td>0</td></tr><tr><td>test_query_class_1_distance</td><td>0.48991</td></tr><tr><td>test_query_class_1_predicted</td><td>1</td></tr><tr><td>test_query_class_2_distance</td><td>0.79085</td></tr><tr><td>test_query_class_2_predicted</td><td>3</td></tr><tr><td>test_query_class_3_distance</td><td>0.76199</td></tr><tr><td>test_query_class_3_predicted</td><td>9</td></tr><tr><td>test_query_class_4_distance</td><td>0.48842</td></tr><tr><td>test_query_class_4_predicted</td><td>4</td></tr><tr><td>test_query_class_5_distance</td><td>0.6259</td></tr><tr><td>test_query_class_5_predicted</td><td>3</td></tr><tr><td>test_query_class_6_distance</td><td>0.57471</td></tr><tr><td>test_query_class_6_predicted</td><td>6</td></tr><tr><td>test_query_class_7_distance</td><td>0.50342</td></tr><tr><td>test_query_class_7_predicted</td><td>7</td></tr><tr><td>test_query_class_8_distance</td><td>0.85735</td></tr><tr><td>test_query_class_8_predicted</td><td>9</td></tr><tr><td>test_query_class_9_distance</td><td>0.56997</td></tr><tr><td>test_query_class_9_predicted</td><td>9</td></tr><tr><td>train_acc</td><td>1</td></tr><tr><td>train_loss</td><td>0.10775</td></tr><tr><td>trainer/global_step</td><td>5940</td></tr><tr><td>val_acc</td><td>0.8681</td></tr><tr><td>val_loss</td><td>0.43251</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">siam.mnist.kan_with_cnn.15</strong> at: <a href='https://wandb.ai/hdu-dk/Few-KAN/runs/ph24xye6' target=\"_blank\">https://wandb.ai/hdu-dk/Few-KAN/runs/ph24xye6</a><br/> View project at: <a href='https://wandb.ai/hdu-dk/Few-KAN' target=\"_blank\">https://wandb.ai/hdu-dk/Few-KAN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241209_100251-ph24xye6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training siam.mnist.kan_basic.15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20241209_100739-f5dhco0y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hdu-dk/Few-KAN/runs/f5dhco0y' target=\"_blank\">siam.mnist.kan_basic.15</a></strong> to <a href='https://wandb.ai/hdu-dk/Few-KAN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hdu-dk/Few-KAN' target=\"_blank\">https://wandb.ai/hdu-dk/Few-KAN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hdu-dk/Few-KAN/runs/f5dhco0y' target=\"_blank\">https://wandb.ai/hdu-dk/Few-KAN/runs/f5dhco0y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/dtu/blackhole/03/213187/kan/env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m/dtu/blackhole/03/213187/kan/env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m/dtu/blackhole/03/213187/kan/env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:943\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mprepare_data()\n\u001b[0;32m--> 943\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_setup_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# allow user to set up LightningModule in accelerator environment\u001b[39;00m\n\u001b[1;32m    944\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: configuring model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/dtu/blackhole/03/213187/kan/env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:102\u001b[0m, in \u001b[0;36m_call_setup_hook\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mdatamodule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[43m_call_lightning_datamodule_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msetup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m _call_callback_hooks(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetup\u001b[39m\u001b[38;5;124m\"\u001b[39m, stage\u001b[38;5;241m=\u001b[39mfn)\n",
      "File \u001b[0;32m/dtu/blackhole/03/213187/kan/env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:189\u001b[0m, in \u001b[0;36m_call_lightning_datamodule_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningDataModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mdatamodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 189\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 147\u001b[0m, in \u001b[0;36mSiameseDataModule.setup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Create Siamese datasets for train and val\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSiameseDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages_per_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages_per_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_pairs_per_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_pairs_per_class\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# For validation, we use the same images_per_class but fewer pairs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mSiameseDataset.__init__\u001b[0;34m(self, dataset, images_per_class, n_pairs_per_class)\u001b[0m\n\u001b[1;32m     13\u001b[0m all_class_indices: Dict[\u001b[38;5;28mint\u001b[39m, List[\u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_class_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dtu/blackhole/03/213187/kan/env/lib/python3.11/site-packages/torch/utils/data/dataset.py:412\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/dtu/blackhole/03/213187/kan/env/lib/python3.11/site-packages/torchvision/datasets/mnist.py:139\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    tuple: (image, target) where target is index of the target class.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msiam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m15\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     58\u001b[0m architectures \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkan_with_cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkan_basic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkkan\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 60\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchitectures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResults Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arch, result \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[9], line 50\u001b[0m, in \u001b[0;36mrun_experiments\u001b[0;34m(base_names, architectures)\u001b[0m\n\u001b[1;32m     43\u001b[0m model \u001b[38;5;241m=\u001b[39m SiameseKANModule(\n\u001b[1;32m     44\u001b[0m     img_size\u001b[38;5;241m=\u001b[39mimg_size,\n\u001b[1;32m     45\u001b[0m     channels\u001b[38;5;241m=\u001b[39mchannels,\n\u001b[1;32m     46\u001b[0m     architecture\u001b[38;5;241m=\u001b[39march\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m results[arch] \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest(model, data_module)\n\u001b[1;32m     53\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/dtu/blackhole/03/213187/kan/env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dtu/blackhole/03/213187/kan/env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "def run_experiments(base_names, architectures):\n",
    "    results = {}\n",
    "    \n",
    "    data_module = SiameseDataModule(\n",
    "        dataset_name=base_names[1],\n",
    "        images_per_class=int(base_names[3]),\n",
    "        n_pairs_per_class=100,\n",
    "        val_split=0.2,\n",
    "        batch_size=16,\n",
    "        img_size=28 if base_names[1] == \"mnist\" else 32\n",
    "    )\n",
    "    \n",
    "    channels = 1 if data_module.dataset_name == \"mnist\" else 3\n",
    "    img_size = 28 if data_module.dataset_name == \"mnist\" else 32\n",
    "    \n",
    "    for arch in architectures:\n",
    "        run_names = base_names.copy()\n",
    "        run_names[2] = arch \n",
    "        run_name = \".\".join(run_names)\n",
    "        \n",
    "        wandb_logger = WandbLogger(\n",
    "            project=\"Few-KAN\",\n",
    "            name=run_name,\n",
    "            group=f\"{base_names[1]}_{base_names[-1]}\"  \n",
    "        )\n",
    "        \n",
    "        trainer = L.Trainer(\n",
    "            max_epochs=50,\n",
    "            callbacks=[\n",
    "                L.pytorch.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=10\n",
    "                ),\n",
    "                L.pytorch.callbacks.ModelCheckpoint(\n",
    "                    monitor='val_loss',\n",
    "                    filename=f'{run_name}-{{epoch:02d}}-{{val_loss:.2f}}',\n",
    "                    save_top_k=1\n",
    "                )\n",
    "            ],\n",
    "            logger=wandb_logger\n",
    "        )\n",
    "        \n",
    "        model = SiameseKANModule(\n",
    "            img_size=img_size,\n",
    "            channels=channels,\n",
    "            architecture=arch\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTraining {run_name}\")\n",
    "        trainer.fit(model, data_module)\n",
    "        results[arch] = trainer.test(model, data_module)\n",
    "        \n",
    "        wandb.finish()\n",
    "    \n",
    "    return results\n",
    "\n",
    "names = [\"siam\", \"mnist\", \"v2\", \"15\"]\n",
    "architectures = [\"kan_with_cnn\", \"kan_basic\", \"kkan\"]\n",
    "\n",
    "results = run_experiments(names, architectures)\n",
    "\n",
    "print(\"\\nResults Summary:\")\n",
    "for arch, result in results.items():\n",
    "    print(f\"\\n{arch}:\")\n",
    "    for metric, value in result[0].items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = [\"siam\", \"mnist\", \"kan_basic\", \"linear_kan_embedding\", \"v1\", \"shot_5\"]\n",
    "# wandb_logger = WandbLogger(project=\"Few-KAN\", name=\".\".join(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SiameseKAN(model_name=names[2], \n",
    "#     channels=1,  # 1 for MNIST, 3 for CIFAR10\n",
    "#     img_size=32,\n",
    "#     hidden_dim=128,\n",
    "#     embedding_dim=64,\n",
    "#     margin=1.0,\n",
    "#     learning_rate=1e-3\n",
    "# )\n",
    "\n",
    "# data_module = SiameseDataModule(\n",
    "#     dataset_name=names[1],  # or \"mnist\"\n",
    "#     images_per_class=10,  # Can be 5, 10, or 15\n",
    "#     n_pairs_per_class=100,\n",
    "#     val_split=0.2,\n",
    "#     batch_size=16,\n",
    "#     img_size=28 if names[1] == \"mnist\" else 32\n",
    "# )\n",
    "# trainer = L.Trainer(\n",
    "#     max_epochs=50,\n",
    "#     callbacks=[\n",
    "#         L.pytorch.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
    "#         L.pytorch.callbacks.ModelCheckpoint(monitor='val_loss')\n",
    "#     ]\n",
    "#     ,\n",
    "#      logger=wandb_logger\n",
    "# )\n",
    "\n",
    "\n",
    "# model = SiameseModule(\n",
    "#     img_size=img_size,\n",
    "#     channels=channels,\n",
    "#     hidden_dim=512,\n",
    "#     embedding_dim=128,\n",
    "#     margin=2.0  # Contrastive loss margin\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.fit(model, data_module)\n",
    "\n",
    "# # Build class prototypes\n",
    "# trainer.test(model, data_module)\n",
    "\n",
    "# trainer = L.Trainer(max_epochs=100, logger=wandb_logger)\n",
    "# trainer.fit(model, data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
